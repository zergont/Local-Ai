# Example environment configuration for Local Responses API

# API server host/port
LOCALAPI_API_HOST=127.0.0.1
LOCALAPI_API_PORT=8080

# SQLite path (created if missing)
LOCALAPI_DATABASE_PATH=data/local_api.db

# Base URL for OpenAI-compatible API (LM Studio)
LOCALAPI_LLM_BASE_URL=http://192.168.0.111:1234/v1

# Default chat model/controller identifier
LOCALAPI_LLM_MODEL=qwen/qwen3-14b

# Vision model
LOCALAPI_VISION_MODEL=qwen2.5-vl-7b-instruct@q8_0

# Generation parameters
LOCALAPI_TEMPERATURE=0.2
LOCALAPI_MAX_TOKENS=512

# Context control
LOCALAPI_MAX_CONTEXT_MESSAGES=12
LOCALAPI_SUMMARIZE_AFTER_MESSAGES=16
# Token budgeting (prompt window management)
LOCALAPI_CONTEXT_WINDOW_TOKENS=32768
LOCALAPI_CONTEXT_PROMPT_BUDGET_RATIO=0.6
LOCALAPI_CONTEXT_HYSTERESIS_TOKENS=1024

# HTTP timeout (seconds)
LOCALAPI_REQUEST_TIMEOUT=60

# Log level
LOCALAPI_LOG_LEVEL=INFO

# Files directory for /file/{id}
LOCALAPI_FILES_DIR=files

# Upload limits and whitelist (overrides available via LOCALAI_*)
LOCALAPI_MAX_UPLOAD_MB=25
LOCALAPI_ALLOWED_EXTS=.png,.jpg,.jpeg,.webp,.gif,.pdf,.txt

# Legacy UI override names (optional)
LOCALAI_MAX_UPLOAD_MB=25
LOCALAI_ALLOWED_EXTS=.png,.jpg,.jpeg,.webp,.gif,.pdf,.txt
